wandb:
  entity: null
  resume: 'auto'

experiment:
    project: "coding_rl" # need to be the same as the file name
    start_from_scratch: True # you can set False to resume training, need to set current_epoch too
    total_step: 300
    save_every: 30
    eval_every: 10
    current_epoch: 1
    deepspeed_file: "4_node_8_gpus_deepspeed_zero3" # align with num_node
    num_node: 4 # the number of machines you have
    node_index: 0 # no need to change


system:
    HTTP_PROXY: null # set proxy if needed, e.g. "http://xxx.xx.xxx.xxx:xxxx"
    HF_HOME: # absolute path of your HF_HOME
    env_name: "rlanything" # env name
    envs_dir: # absolute path of your env directory
    rl_base_dir: # /absolute/path/to/Open-AgentRL


model:
    policy_model: # absolute path of your policy model
    reward_model: # absolute path of your reward model
    environment_model: # absolute path of your environment model
    optimized_name: "optimized"
    optimized_reward_name: "optimized_reward"


dataset:
    environment_type: "stem_tasks"
    train_dataset: "CodeContests_train" 
    eval_dataset: "LiveBench-ReasonFlux"
    dataset_state_prefix: "dataset_state"
    env_pending_prefix: "env_pending"
    optimization_data: "rl_data"
    reward_optimization_data: "reward_rl_data"

execute:
    num_chunk: 128 # batch size of executing codes in coding eval tasks

rollout:
    policy:
      num_task: 64
      num_response_per_task: 32
      temperature: 1.0
      model_length: 10000
      max_gen_length: 2048
      gpu_groups: [[0,1,2,3],[4,5,6,7]]
      start_with_think: False
    reward:
      num_response_per_task: 16
      temperature: 0.8
      model_length: 10000
      max_gen_length: 2048
      gpu_groups: [[0,1,2,3],[4,5,6,7]]
      start_with_think: False
    environment:
        temperature: 0.8
        num_response_per_task: 3
        model_length: 20000
        max_gen_length: 8096
        gpu_groups: [[0,1,2,3],[4,5,6,7]]
        if_start_with_think: True



training:
    policy:
        gradient_checkpointing_enable: False
        update_per_step: 2
        batch_size_lm: 1
        max_gen_length: 2048
        max_prompt_len: 2048
    reward:
        gradient_checkpointing_enable: False
        update_per_step: 2
        batch_size_lm: 1
        max_gen_length: 4096
        max_prompt_len: 2048
    mixed_precision: "bf16"
    enable_tf32: True
    seed: 10086
    max_grad_norm: 1.0
    use_kl_estimator_k3: True
    eps: 0.20
    beta: 0.01
    num_train_epochs: 1


optimizer:
    name: adamw
    params: # default adamw params
        learning_rate: 1e-6
        scale_lr: False # scale learning rate by total batch size
        beta1: 0.9
        beta2: 0.999
        weight_decay: 0.0
        epsilon: 1e-8


lr_scheduler:
    scheduler: "cosine"
    params:
        learning_rate: ${optimizer.params.learning_rate}
        warmup_steps: 0
        min_lr_scale: 1.0


evaluation:
    policy:
        num_response_per_task: 3
        temperature: 0.0
        model_length: 20000
        max_gen_length: 2000
        gpu_groups: [[0,1,2,3],[4,5,6,7]]
        if_start_with_think: False

